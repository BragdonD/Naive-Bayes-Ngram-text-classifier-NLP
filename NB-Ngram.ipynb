{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "import string\n",
    "PUNCTUATION = string.punctuation\n",
    "\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE_PATH = \"\"\n",
    "JSON_FILE_NAME = \"News_Category_Dataset_v3_balanced.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractJsonData(jsonData):\n",
    "  return pd.read_json(jsonData, lines=True);\n",
    "\n",
    "jsonFile = open(JSON_FILE_PATH + JSON_FILE_NAME);\n",
    "df = extractJsonData(jsonFile);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('link', axis=1)\n",
    "df = df.drop('authors', axis=1)\n",
    "df = df.drop('date', axis=1)\n",
    "df = df.drop('headline', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProcessor:\n",
    "  def __init__(self, language=\"english\", lemmatization = False, stopword = False, stemmatization = False, lower = False, ponct = False, emoji = False, symbols = False, numbers = False):\n",
    "    if (lemmatization == True & stemmatization == True):\n",
    "      raise Exception(\"Can not lemmatize and stem sentences at the same time.\")\n",
    "\n",
    "    self.lemmatization = lemmatization\n",
    "    self.stemmatization = stemmatization\n",
    "    self.lower = lower\n",
    "    self.ponct = ponct\n",
    "    self.emoji = emoji\n",
    "    self.stopword = stopword\n",
    "    self.symbols = symbols\n",
    "    self.numbers = numbers\n",
    "    self.lemmatizer = spacy.load('en_core_web_sm')\n",
    "    self.stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    self.REPLACE_BY_SPACE_RE = re.compile('[-+/(){}\\[\\]\\|@,;]')\n",
    "    self.BAD_SYMBOLS_RE = re.compile('[0-9] {,1}')\n",
    "    self.STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "  def cleanText(self, text):\n",
    "    if text == \"\":\n",
    "      return \"\"\n",
    "\n",
    "    def lower_case(text):\n",
    "      return text.lower()\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "      return text.translate(str.maketrans('', '', PUNCTUATION))\n",
    "\n",
    "    def remove_symbols(dataframe):\n",
    "      return self.REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "    def remove_numbers(text):\n",
    "      return self.BAD_SYMBOLS_RE.sub(' ', text)\n",
    "\n",
    "    def remove_emoji(string):\n",
    "      emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "      return emoji_pattern.sub(' ', string)\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "      return \" \".join([word for word in str(text).split() if word not in self.STOPWORDS])\n",
    "\n",
    "    def lemmatize(text):\n",
    "      tokens = []\n",
    "      for token in self.lemmatizer(text):\n",
    "        tokens.append(token.lemma_)\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "    def stemmatize(text):\n",
    "      tokens = []\n",
    "      for token in text.split(\" \"):\n",
    "        tokens.append(self.stemmer.stem(token))\n",
    "      return \" \".join(tokens)\n",
    "        \n",
    "    if(self.lower == True):\n",
    "      text = lower_case(text)\n",
    "    if(self.numbers == True):\n",
    "      text = remove_numbers(text)\n",
    "    if(self.ponct == True):\n",
    "      text = remove_punctuation(text)\n",
    "    if(self.symbols == True):\n",
    "      text = remove_symbols(text)\n",
    "    if(self.emoji == True):\n",
    "      text = remove_emoji(text)\n",
    "    if(self.stopword == True):\n",
    "      text = remove_stopwords(text)\n",
    "    if(self.lemmatization == True):\n",
    "      text = lemmatize(text)\n",
    "    if(self.stemmatization == True):\n",
    "      text = stemmatize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "    def __init__(self, n=1):\n",
    "        self.n = n\n",
    "        self.lemm = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def ngram(self, text):\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            words.append(word)\n",
    "        temp = zip(*[words[i:] for i in range(0, self.n)])\n",
    "        ans = [' '.join(n) for n in temp]\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nb_ngram:\n",
    "    def __init__(self, textPreProcessor = TextPreProcessor(), ngram=1, preProcess = False):\n",
    "        self.nbClass = 0\n",
    "        self.isCompile = False\n",
    "        self.isTrain = False\n",
    "        self.BoT = dict()\n",
    "        self.classesProb = []\n",
    "        self.nGram = NGram(ngram)\n",
    "        self.preProcess = preProcess\n",
    "        self.textPreProcessor = textPreProcessor\n",
    "        \n",
    "    def compile(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.classes = np.unique(Y)\n",
    "        self.nbClass = len(self.classes)\n",
    "        \n",
    "        if self.preProcess == True:\n",
    "            print(\"Preprocessing text:\")\n",
    "            for i, sen in enumerate(tqdm(self.X)):\n",
    "                self.X[i] = self.textPreProcessor.cleanText(sen)\n",
    "\n",
    "        def create_bag_of_word(X, Y):\n",
    "            bags_of_ngram = dict();\n",
    "            for i in self.classes:\n",
    "                bags_of_ngram[i] = dict()\n",
    "            print(\"Creating bags of words...\")\n",
    "            for lab, sen in tqdm(zip(Y, X)):\n",
    "                ngram_sentence = self.nGram.ngram(sen)\n",
    "                for t in ngram_sentence:\n",
    "                    if t not in bags_of_ngram[lab]:\n",
    "                        bags_of_ngram[lab][t] = 0\n",
    "                    bags_of_ngram[lab][t] += 1\n",
    "            return bags_of_ngram\n",
    "\n",
    "        self.BoT = create_bag_of_word(self.X, self.Y)\n",
    "\n",
    "    def get_classes_probabilites_log(self, Y):\n",
    "        def get_classes_occurences(Y):\n",
    "            classes = dict()\n",
    "            for cl in Y:\n",
    "                if cl not in classes:\n",
    "                    classes[cl] = 0\n",
    "                classes[cl] += 1\n",
    "            return classes\n",
    "        \n",
    "        def get_classes_proba_log(classes, nb_samples):\n",
    "            classes_occ = dict()\n",
    "            for cl, occ in classes.items():\n",
    "                classes_occ[cl] = math.log(float(occ) / float(nb_samples))\n",
    "            return classes_occ\n",
    "        \n",
    "        self.classes_proba = get_classes_proba_log( classes = get_classes_occurences(Y), nb_samples = len(Y) )\n",
    "\n",
    "    def train(self):\n",
    "        self.words_by_classes = dict();\n",
    "        self.vocab = []\n",
    "        print(\"Extracting vocab:\")\n",
    "        for cl, dic in tqdm(self.BoT.items()):\n",
    "            if cl not in self.words_by_classes:\n",
    "                self.words_by_classes[cl] = 0\n",
    "            for tok, val in dic.items():\n",
    "                self.words_by_classes[cl] += val\n",
    "                self.vocab.append(tok)\n",
    "\n",
    "        self.vocab = np.unique(self.vocab)\n",
    "        self.vocab_len = len(self.vocab)\n",
    "\n",
    "        self.get_classes_probabilites_log(self.Y)\n",
    "        \n",
    "        self.denominators = dict()\n",
    "        print(\"Calculating classes denominators for probabilites:\")\n",
    "        for cl in tqdm(self.classes):\n",
    "            self.denominators[cl]  = self.words_by_classes[cl] + self.vocab_len\n",
    "        \n",
    "        self.Y_info = [(self.BoT[cl], self.classes_proba[cl], self.denominators[cl]) for cl in self.classes] \n",
    "        self.Y_info = np.array(self.Y_info) \n",
    "\n",
    "    def predict(self, text):\n",
    "        likelihood_prob = np.zeros(self.classes.shape[0])\n",
    "        if self.preProcess == True:\n",
    "            test_str = self.textPreProcessor.cleanText(text)\n",
    "        for cl_i, cl in enumerate(self.classes):                 \n",
    "            for tok in self.nGram.ngram(text):                        \n",
    "                tok_counts = self.Y_info[cl_i][0].get(tok, 0) + 1 # We add 1 due to the formula to not get 0 probabilities                        \n",
    "                tok_prob = tok_counts/float(self.Y_info[cl_i][2])                              \n",
    "                likelihood_prob[cl_i] += math.log(tok_prob)\n",
    "                                                \n",
    "        post_prob = np.empty(self.classes.shape[0])\n",
    "        for cl_i, cl in enumerate(self.classes):\n",
    "            post_prob[cl_i] = likelihood_prob[cl_i] + self.Y_info[cl_i][1]                              \n",
    "        \n",
    "        return post_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "textCleaner = TextPreProcessor( \n",
    "    lemmatization=False,\n",
    "    lower=True,\n",
    "    stopword = False, \n",
    "    stemmatization = True, \n",
    "    ponct = True, \n",
    "    emoji = True, \n",
    "    symbols = True, \n",
    "    numbers = True)\n",
    "nb = Nb_ngram(ngram=3, textPreProcessor=textCleaner, preProcess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    shuffle=True, \n",
    "    stratify=df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167621/167621 [00:31<00:00, 5371.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bags of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167621it [00:02, 63978.71it/s]\n"
     ]
    }
   ],
   "source": [
    "nb.compile(train.short_description.values, train.category.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting vocab:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 54.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating classes denominators for probabilites:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X,Y):\n",
    "    true_possitive = 0\n",
    "    for lb, text in tqdm(zip(Y, X), total=len(Y)):\n",
    "        predict = nb.classes[nb.predict(text).argmax()]\n",
    "        if predict == lb:\n",
    "            true_possitive += 1\n",
    "\n",
    "    return float(true_possitive) / float(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve better accuracy when we do not remove the stop words. The accuracy is at its best when ngram=3.\n",
    "The accuracy is still pretty low due to an umbalanced dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41906/41906 [00:41<00:00, 1021.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is : 0.18049921252326637 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(test.short_description.values, test.category.values)\n",
    "print(f\"Model accuracy is : {accuracy} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f56dc8f305b8407d8b48cc33503d84dbeabbbd03fe8c646d69c42e169c35af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
