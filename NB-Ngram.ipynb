{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "import string\n",
    "PUNCTUATION = string.punctuation\n",
    "\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE_PATH = \"\"\n",
    "JSON_FILE_NAME = \"News_Category_Dataset_v3_balanced.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractJsonData(jsonData):\n",
    "  return pd.read_json(jsonData, lines=True);\n",
    "\n",
    "jsonFile = open(JSON_FILE_PATH + JSON_FILE_NAME);\n",
    "df = extractJsonData(jsonFile);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('link', axis=1)\n",
    "df = df.drop('authors', axis=1)\n",
    "df = df.drop('date', axis=1)\n",
    "df = df.drop('headline', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProcessor:\n",
    "  def __init__(self, language=\"english\", lemmatization = False, stopword = False, stemmatization = False, lower = False, ponct = False, emoji = False, symbols = False, numbers = False):\n",
    "    if (lemmatization == True & stemmatization == True):\n",
    "      raise Exception(\"Can not lemmatize and stem sentences at the same time.\")\n",
    "\n",
    "    self.lemmatization = lemmatization\n",
    "    self.stemmatization = stemmatization\n",
    "    self.lower = lower\n",
    "    self.ponct = ponct\n",
    "    self.emoji = emoji\n",
    "    self.stopword = stopword\n",
    "    self.symbols = symbols\n",
    "    self.numbers = numbers\n",
    "    self.lemmatizer = spacy.load('en_core_web_sm')\n",
    "    self.stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    self.REPLACE_BY_SPACE_RE = re.compile('[-+/(){}\\[\\]\\|@,;]')\n",
    "    self.BAD_SYMBOLS_RE = re.compile('[0-9] {,1}')\n",
    "    self.STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "  def cleanText(self, text):\n",
    "    if text == \"\":\n",
    "      return \"\"\n",
    "\n",
    "    def lower_case(text):\n",
    "      return text.lower()\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "      return text.translate(str.maketrans('', '', PUNCTUATION))\n",
    "\n",
    "    def remove_symbols(dataframe):\n",
    "      return self.REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "    def remove_numbers(text):\n",
    "      return self.BAD_SYMBOLS_RE.sub(' ', text)\n",
    "\n",
    "    def remove_emoji(string):\n",
    "      emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "      return emoji_pattern.sub(' ', string)\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "      return \" \".join([word for word in str(text).split() if word not in self.STOPWORDS])\n",
    "\n",
    "    def lemmatize(text):\n",
    "      tokens = []\n",
    "      for token in self.lemmatizer(text):\n",
    "        tokens.append(token.lemma_)\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "    def stemmatize(text):\n",
    "      tokens = []\n",
    "      for token in text.split(\" \"):\n",
    "        tokens.append(self.stemmer.stem(token))\n",
    "      return \" \".join(tokens)\n",
    "        \n",
    "    if(self.lower == True):\n",
    "      text = lower_case(text)\n",
    "    if(self.numbers == True):\n",
    "      text = remove_numbers(text)\n",
    "    if(self.ponct == True):\n",
    "      text = remove_punctuation(text)\n",
    "    if(self.symbols == True):\n",
    "      text = remove_symbols(text)\n",
    "    if(self.emoji == True):\n",
    "      text = remove_emoji(text)\n",
    "    if(self.stopword == True):\n",
    "      text = remove_stopwords(text)\n",
    "    if(self.lemmatization == True):\n",
    "      text = lemmatize(text)\n",
    "    if(self.stemmatization == True):\n",
    "      text = stemmatize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "    def __init__(self, n=1):\n",
    "        self.n = n\n",
    "\n",
    "    def ngram(self, text):\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            words.append(word)\n",
    "        temp = zip(*[words[i:] for i in range(0, self.n)])\n",
    "        ans = [' '.join(n) for n in temp]\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nb_ngram:\n",
    "    def __init__(self, textPreProcessor = TextPreProcessor(), ngram=1, preProcess = False):\n",
    "        self.nbClass = 0\n",
    "        self.isCompile = False\n",
    "        self.isTrain = False\n",
    "        self.BoT = dict()\n",
    "        self.classesProb = []\n",
    "        self.nGram = NGram(ngram)\n",
    "        self.preProcess = preProcess\n",
    "        self.textPreProcessor = textPreProcessor\n",
    "    \n",
    "    def get_classes_occurences(self, Y):\n",
    "        classes = dict()\n",
    "        for cl in Y:\n",
    "            if cl not in classes:\n",
    "                classes[cl] = 0\n",
    "            classes[cl] += 1\n",
    "        return classes\n",
    "        \n",
    "    def compile(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.classes = np.unique(Y)\n",
    "        self.nbClass = len(self.classes)\n",
    "        \n",
    "        if self.preProcess == True:\n",
    "            print(\"Preprocessing text:\")\n",
    "            for i, sen in enumerate(tqdm(self.X)):\n",
    "                self.X[i] = self.textPreProcessor.cleanText(sen)\n",
    "\n",
    "        def oversampling(X, Y):\n",
    "            average_input_size = 0\n",
    "            for val in X:\n",
    "                average_input_size += len(val.split(\" \"))\n",
    "            average_input_size = int(average_input_size / len(X))\n",
    "            \n",
    "            max_size = 0\n",
    "            classes_occ = self.get_classes_occurences(Y)\n",
    "            for cl, occ in classes_occ.items():\n",
    "                if occ > max_size:\n",
    "                    max_size = occ\n",
    "\n",
    "            for classe in np.unique(Y):\n",
    "                classe_vocab = []\n",
    "                for text, lb in zip(X, Y):\n",
    "                    if classe == lb:\n",
    "                        classe_vocab += self.nGram.ngram(text)\n",
    "                classe_vocab = np.unique(classe_vocab)\n",
    "                gen_nb = max_size - classes_occ[classe]\n",
    "                print(f\"Oversampling the classe {classe}:\")\n",
    "                arr = []\n",
    "                classes = []\n",
    "                for i in tqdm(range(gen_nb)):\n",
    "                    random_tokens = np.random.choice(classe_vocab, size=int(average_input_size / self.nGram.n), replace=True)\n",
    "                    new_string = \"\"\n",
    "                    index = 0\n",
    "                    for i in random_tokens:\n",
    "                        new_string += i \n",
    "                        if index != max_size-1:\n",
    "                            new_string += \" \"\n",
    "                        index += 1\n",
    "                    arr.append(new_string)\n",
    "                    classes.append(classe)\n",
    "\n",
    "                self.X = np.append(self.X, arr)\n",
    "                self.Y = np.append(self.Y, classes)\n",
    "\n",
    "        oversampling(self.X, self.Y)\n",
    "\n",
    "        def create_bag_of_word(X, Y):\n",
    "            bags_of_ngram = dict();\n",
    "            for i in self.classes:\n",
    "                bags_of_ngram[i] = dict()\n",
    "            print(\"Creating bags of words...\")\n",
    "            for lab, sen in tqdm(zip(Y, X)):\n",
    "                ngram_sentence = self.nGram.ngram(sen)\n",
    "                for t in ngram_sentence:\n",
    "                    if t not in bags_of_ngram[lab]:\n",
    "                        bags_of_ngram[lab][t] = 0\n",
    "                    bags_of_ngram[lab][t] += 1\n",
    "            return bags_of_ngram\n",
    "\n",
    "        self.BoT = create_bag_of_word(self.X, self.Y)\n",
    "\n",
    "    def get_classes_probabilites_log(self, Y):\n",
    "        def get_classes_proba_log(classes, nb_samples):\n",
    "            classes_occ = dict()\n",
    "            for cl, occ in classes.items():\n",
    "                classes_occ[cl] = math.log(float(occ) / float(nb_samples))\n",
    "            return classes_occ\n",
    "        \n",
    "        self.classes_proba = get_classes_proba_log( classes = self.get_classes_occurences(Y), nb_samples = len(Y) )\n",
    "\n",
    "    def train(self):\n",
    "        self.words_by_classes = dict();\n",
    "        self.vocab = dict()\n",
    "        print(\"Extracting vocab:\")\n",
    "        for cl, dic in self.BoT.items():\n",
    "            if cl not in self.words_by_classes:\n",
    "                self.words_by_classes[cl] = 0\n",
    "            for tok, val in tqdm(dic.items()):\n",
    "                self.words_by_classes[cl] += val\n",
    "                self.vocab[tok] = 1\n",
    "        print(len(self.vocab))\n",
    "        self.vocab_len = len(self.vocab)\n",
    "\n",
    "        self.get_classes_probabilites_log(self.Y)\n",
    "        \n",
    "        self.denominators = dict()\n",
    "        print(\"Calculating classes denominators for probabilites:\")\n",
    "        for cl in tqdm(self.classes):\n",
    "            self.denominators[cl]  = self.words_by_classes[cl] + self.vocab_len\n",
    "        \n",
    "        self.Y_info = [(self.BoT[cl], self.classes_proba[cl], self.denominators[cl]) for cl in self.classes] \n",
    "        self.Y_info = np.array(self.Y_info) \n",
    "\n",
    "    def predict(self, text):\n",
    "        likelihood_prob = np.zeros(self.classes.shape[0])\n",
    "        if self.preProcess == True:\n",
    "            text = self.textPreProcessor.cleanText(text)\n",
    "        for cl_i, cl in enumerate(self.classes):                 \n",
    "            for tok in self.nGram.ngram(text):                        \n",
    "                tok_counts = self.Y_info[cl_i][0].get(tok, 0) + 1 # We add 1 due to the formula to not get 0 probabilities                        \n",
    "                tok_prob = tok_counts/float(self.Y_info[cl_i][2])                              \n",
    "                likelihood_prob[cl_i] += math.log(tok_prob)\n",
    "                                                \n",
    "        post_prob = np.empty(self.classes.shape[0])\n",
    "        for cl_i, cl in enumerate(self.classes):\n",
    "            post_prob[cl_i] = likelihood_prob[cl_i] + self.Y_info[cl_i][1]                              \n",
    "        \n",
    "        return post_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "textCleaner = TextPreProcessor( \n",
    "    lemmatization=False,\n",
    "    lower=True,\n",
    "    stopword = False, \n",
    "    stemmatization = True, \n",
    "    ponct = True, \n",
    "    emoji = True, \n",
    "    symbols = True, \n",
    "    numbers = True)\n",
    "nb = Nb_ngram(ngram=3, textPreProcessor=textCleaner, preProcess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    shuffle=True, \n",
    "    stratify=df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167621/167621 [00:28<00:00, 5808.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe ARTS & CULTURE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25343/25343 [00:00<00:00, 39848.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe BUSINESS & FINANCES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22283/22283 [00:00<00:00, 39720.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe COMEDY:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24161/24161 [00:00<00:00, 40200.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe CRIME:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25631/25631 [00:00<00:00, 40363.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe DIVORCE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25740/25740 [00:00<00:00, 39845.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe EDUCATION:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26755/26755 [00:00<00:00, 39521.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe ENTERTAINMENT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14592/14592 [00:00<00:00, 39330.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe ENVIRONMENT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25228/25228 [00:00<00:00, 39604.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe FOOD & DRINK:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21732/21732 [00:00<00:00, 39300.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe GROUPS VOICES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18833/18833 [00:00<00:00, 39904.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe HOME & LIVING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25025/25025 [00:00<00:00, 40493.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe IMPACT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25694/25694 [00:00<00:00, 40272.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe MEDIA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26126/26126 [00:00<00:00, 38764.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe MISCELLANEOUS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26242/26242 [00:00<00:00, 39515.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe PARENTING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18284/18284 [00:00<00:00, 37311.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe POLITICS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe RELIGION:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26419/26419 [00:00<00:00, 40026.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe SCIENCE & TECH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25033/25033 [00:00<00:00, 39610.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe SPORTS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24419/24419 [00:00<00:00, 40096.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe STYLE & BEAUTY:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18827/18827 [00:00<00:00, 39060.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe TRAVEL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20561/20561 [00:00<00:00, 37451.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe U.S. NEWS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27379/27379 [00:00<00:00, 40800.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WEDDINGS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25559/25559 [00:00<00:00, 39751.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WEIRD NEWS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26259/26259 [00:00<00:00, 41026.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WELLNESS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8770/8770 [00:00<00:00, 37799.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WOMEN:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25623/25623 [00:00<00:00, 40101.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WORLD NEWS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20848/20848 [00:00<00:00, 39408.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bags of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768987it [00:10, 73912.56it/s]\n"
     ]
    }
   ],
   "source": [
    "nb.compile(train.short_description.values, train.category.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting vocab:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294030/294030 [00:00<00:00, 1872406.54it/s]\n",
      "100%|██████████| 331379/331379 [00:00<00:00, 1416527.91it/s]\n",
      "100%|██████████| 274627/274627 [00:00<00:00, 1352625.85it/s]\n",
      "100%|██████████| 274565/274565 [00:00<00:00, 1605726.07it/s]\n",
      "100%|██████████| 299689/299689 [00:00<00:00, 871164.55it/s] \n",
      "100%|██████████| 289995/289995 [00:00<00:00, 1812224.74it/s]\n",
      "100%|██████████| 273364/273364 [00:00<00:00, 1697938.05it/s]\n",
      "100%|██████████| 297555/297555 [00:00<00:00, 1377416.90it/s]\n",
      "100%|██████████| 298647/298647 [00:00<00:00, 1507993.50it/s]\n",
      "100%|██████████| 318060/318060 [00:00<00:00, 1309101.94it/s]\n",
      "100%|██████████| 288098/288098 [00:00<00:00, 789859.56it/s]\n",
      "100%|██████████| 305496/305496 [00:00<00:00, 1558680.71it/s]\n",
      "100%|██████████| 275773/275773 [00:00<00:00, 1641191.69it/s]\n",
      "100%|██████████| 291912/291912 [00:00<00:00, 1586500.74it/s]\n",
      "100%|██████████| 382683/382683 [00:00<00:00, 1530978.75it/s]\n",
      "100%|██████████| 325936/325936 [00:00<00:00, 1490162.05it/s]\n",
      "100%|██████████| 280257/280257 [00:00<00:00, 1490662.85it/s]\n",
      "100%|██████████| 287183/287183 [00:00<00:00, 1527290.53it/s]\n",
      "100%|██████████| 277015/277015 [00:00<00:00, 1364334.65it/s]\n",
      "100%|██████████| 315177/315177 [00:00<00:00, 1260418.31it/s]\n",
      "100%|██████████| 365252/365252 [00:00<00:00, 1404590.04it/s]\n",
      "100%|██████████| 275150/275150 [00:00<00:00, 576825.73it/s]\n",
      "100%|██████████| 301605/301605 [00:00<00:00, 1603936.43it/s]\n",
      "100%|██████████| 261003/261003 [00:00<00:00, 1449683.94it/s]\n",
      "100%|██████████| 476623/476623 [00:00<00:00, 1508084.52it/s]\n",
      "100%|██████████| 290781/290781 [00:00<00:00, 1538524.69it/s]\n",
      "100%|██████████| 303421/303421 [00:00<00:00, 1622305.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7107966\n",
      "Calculating classes denominators for probabilites:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "nb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X,Y):\n",
    "    true_positive = 0\n",
    "    for lb, text in tqdm(zip(Y, X), total=len(Y)):\n",
    "        predict = nb.classes[nb.predict(text).argmax()]\n",
    "        if predict == lb:\n",
    "            true_positive += 1\n",
    "\n",
    "    return float(true_positive) / float(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve better accuracy when we do not remove the stop words. The accuracy is at its best when ngram=3.\n",
    "The accuracy is still pretty low due to an umbalanced dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41906/41906 [00:39<00:00, 1064.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is : 0.2937765475110963 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(test.short_description.values, test.category.values)\n",
    "print(f\"Model accuracy is : {accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-346.57262498, -344.07186453, -348.42805305, -350.3331388 ,\n",
       "       -346.3997935 , -350.68664783, -348.35660948, -346.78740049,\n",
       "       -350.63540046, -337.94696869, -349.81514548, -345.5925091 ,\n",
       "       -351.72785539, -345.67120572, -346.472421  , -337.15574614,\n",
       "       -350.37001604, -348.40851679, -343.51189011, -348.60718015,\n",
       "       -345.48115296, -350.65537456, -350.20389568, -350.99945142,\n",
       "       -348.02089534, -345.00590415, -341.41783709])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(\"FIFA has come under pressure from several European soccer federations who want to support a human rights campaign against discrimination at the World Cup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIFA has come under pressure from several European soccer federations who want to support a human rights campaign against discrimination at the World Cup. WORLD NEWS\n"
     ]
    }
   ],
   "source": [
    "print(df.short_description[10],df.category[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                  short_description\n",
       "0  U.S. NEWS  Health experts said it is too early to predict...\n",
       "1  U.S. NEWS  He was subdued by passengers and crew when he ...\n",
       "2     COMEDY  \"Until you have a dog you don't understand wha...\n",
       "3  PARENTING  \"Accidentally put grown-up toothpaste on my to...\n",
       "4  U.S. NEWS  Amy Cooper accused investment firm Franklin Te..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotlight this week togeth to make import part of to think in to the day out of mess  EDUCATION\n"
     ]
    }
   ],
   "source": [
    "print(nb.X[300000], nb.Y[300000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f56dc8f305b8407d8b48cc33503d84dbeabbbd03fe8c646d69c42e169c35af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
