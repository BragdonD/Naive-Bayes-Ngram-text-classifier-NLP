{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\33610\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "\n",
    "import string\n",
    "PUNCTUATION = string.punctuation\n",
    "\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_FILE_PATH = \"\"\n",
    "JSON_FILE_NAME = \"News_Category_Dataset_v3_balanced.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractJsonData(jsonData):\n",
    "  return pd.read_json(jsonData, lines=True);\n",
    "\n",
    "jsonFile = open(JSON_FILE_PATH + JSON_FILE_NAME);\n",
    "df = extractJsonData(jsonFile);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('link', axis=1)\n",
    "df = df.drop('authors', axis=1)\n",
    "df = df.drop('date', axis=1)\n",
    "df = df.drop('headline', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreProcessor:\n",
    "  def __init__(self, language=\"english\", lemmatization = False, stopword = False, stemmatization = False, lower = False, ponct = False, emoji = False, symbols = False, numbers = False):\n",
    "    if (lemmatization == True & stemmatization == True):\n",
    "      raise Exception(\"Can not lemmatize and stem sentences at the same time.\")\n",
    "\n",
    "    self.lemmatization = lemmatization\n",
    "    self.stemmatization = stemmatization\n",
    "    self.lower = lower\n",
    "    self.ponct = ponct\n",
    "    self.emoji = emoji\n",
    "    self.stopword = stopword\n",
    "    self.symbols = symbols\n",
    "    self.numbers = numbers\n",
    "    self.lemmatizer = spacy.load('en_core_web_sm')\n",
    "    self.stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    self.REPLACE_BY_SPACE_RE = re.compile('[-+/(){}\\[\\]\\|@,;]')\n",
    "    self.BAD_SYMBOLS_RE = re.compile('[0-9] {,1}')\n",
    "    self.STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "  def cleanText(self, text):\n",
    "    if text == \"\":\n",
    "      return \"\"\n",
    "\n",
    "    def lower_case(text):\n",
    "      return text.lower()\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "      return text.translate(str.maketrans('', '', PUNCTUATION))\n",
    "\n",
    "    def remove_symbols(dataframe):\n",
    "      return self.REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "    def remove_numbers(text):\n",
    "      return self.BAD_SYMBOLS_RE.sub(' ', text)\n",
    "\n",
    "    def remove_emoji(string):\n",
    "      emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "      return emoji_pattern.sub(' ', string)\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "      return \" \".join([word for word in str(text).split() if word not in self.STOPWORDS])\n",
    "\n",
    "    def lemmatize(text):\n",
    "      tokens = []\n",
    "      for token in self.lemmatizer(text):\n",
    "        tokens.append(token.lemma_)\n",
    "      return \" \".join(tokens)\n",
    "\n",
    "    def stemmatize(text):\n",
    "      tokens = []\n",
    "      for token in text.split(\" \"):\n",
    "        tokens.append(self.stemmer.stem(token))\n",
    "      return \" \".join(tokens)\n",
    "        \n",
    "    if(self.lower == True):\n",
    "      text = lower_case(text)\n",
    "    if(self.numbers == True):\n",
    "      text = remove_numbers(text)\n",
    "    if(self.ponct == True):\n",
    "      text = remove_punctuation(text)\n",
    "    if(self.symbols == True):\n",
    "      text = remove_symbols(text)\n",
    "    if(self.emoji == True):\n",
    "      text = remove_emoji(text)\n",
    "    if(self.stopword == True):\n",
    "      text = remove_stopwords(text)\n",
    "    if(self.lemmatization == True):\n",
    "      text = lemmatize(text)\n",
    "    if(self.stemmatization == True):\n",
    "      text = stemmatize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "    def __init__(self, n=1):\n",
    "        self.n = n\n",
    "\n",
    "    def ngram(self, text):\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            words.append(word)\n",
    "        temp = zip(*[words[i:] for i in range(0, self.n)])\n",
    "        ans = [' '.join(n) for n in temp]\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nb_ngram:\n",
    "    def __init__(self, textPreProcessor = TextPreProcessor(), ngram=1, preProcess = False):\n",
    "        self.nbClass = 0\n",
    "        self.isCompile = False\n",
    "        self.isTrain = False\n",
    "        self.BoT = dict()\n",
    "        self.classesProb = []\n",
    "        self.nGram = NGram(ngram)\n",
    "        self.preProcess = preProcess\n",
    "        self.textPreProcessor = textPreProcessor\n",
    "    \n",
    "    def get_classes_occurences(self, Y):\n",
    "        classes = dict()\n",
    "        for cl in Y:\n",
    "            if cl not in classes:\n",
    "                classes[cl] = 0\n",
    "            classes[cl] += 1\n",
    "        return classes\n",
    "        \n",
    "    def compile(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.classes = np.unique(Y)\n",
    "        self.nbClass = len(self.classes)\n",
    "        \n",
    "        if self.preProcess == True:\n",
    "            print(\"Preprocessing text:\")\n",
    "            for i, sen in enumerate(tqdm(self.X)):\n",
    "                self.X[i] = self.textPreProcessor.cleanText(sen)\n",
    "\n",
    "        def oversampling(X, Y):\n",
    "            average_input_size = 0\n",
    "            for val in X:\n",
    "                average_input_size += len(val.split(\" \"))\n",
    "            average_input_size = int(average_input_size / len(X))\n",
    "            \n",
    "            max_size = 0\n",
    "            classes_occ = self.get_classes_occurences(Y)\n",
    "            for cl, occ in classes_occ.items():\n",
    "                if occ > max_size:\n",
    "                    max_size = occ\n",
    "\n",
    "            for classe in np.unique(Y):\n",
    "                classe_vocab = []\n",
    "                for text, lb in zip(X, Y):\n",
    "                    if classe == lb:\n",
    "                        classe_vocab += self.nGram.ngram(text)\n",
    "                classe_vocab = np.unique(classe_vocab)\n",
    "                gen_nb = max_size - classes_occ[classe]\n",
    "                print(f\"Oversampling the classe {classe}:\")\n",
    "                arr = []\n",
    "                classes = []\n",
    "                for i in tqdm(range(gen_nb)):\n",
    "                    random_tokens = np.random.choice(classe_vocab, size=int(average_input_size / self.nGram.n), replace=True)\n",
    "                    new_string = \"\"\n",
    "                    index = 0\n",
    "                    for i in random_tokens:\n",
    "                        new_string += i \n",
    "                        if index != max_size-1:\n",
    "                            new_string += \" \"\n",
    "                        index += 1\n",
    "                    arr.append(new_string)\n",
    "                    classes.append(classe)\n",
    "\n",
    "                self.X = np.append(self.X, arr)\n",
    "                self.Y = np.append(self.Y, classes)\n",
    "\n",
    "        oversampling(self.X, self.Y)\n",
    "\n",
    "        def create_bag_of_word(X, Y):\n",
    "            bags_of_ngram = dict();\n",
    "            for i in self.classes:\n",
    "                bags_of_ngram[i] = dict()\n",
    "            print(\"Creating bags of words...\")\n",
    "            for lab, sen in tqdm(zip(Y, X)):\n",
    "                ngram_sentence = self.nGram.ngram(sen)\n",
    "                for t in ngram_sentence:\n",
    "                    if t not in bags_of_ngram[lab]:\n",
    "                        bags_of_ngram[lab][t] = 0\n",
    "                    bags_of_ngram[lab][t] += 1\n",
    "            return bags_of_ngram\n",
    "\n",
    "        self.BoT = create_bag_of_word(self.X, self.Y)\n",
    "\n",
    "    def get_classes_probabilites_log(self, Y):\n",
    "        def get_classes_proba_log(classes, nb_samples):\n",
    "            classes_occ = dict()\n",
    "            for cl, occ in classes.items():\n",
    "                classes_occ[cl] = math.log(float(occ) / float(nb_samples))\n",
    "            return classes_occ\n",
    "        \n",
    "        self.classes_proba = get_classes_proba_log( classes = self.get_classes_occurences(Y), nb_samples = len(Y) )\n",
    "\n",
    "    def train(self):\n",
    "        self.words_by_classes = dict();\n",
    "        self.vocab = dict()\n",
    "        print(\"Extracting vocab:\")\n",
    "        for cl, dic in self.BoT.items():\n",
    "            if cl not in self.words_by_classes:\n",
    "                self.words_by_classes[cl] = 0\n",
    "            for tok, val in tqdm(dic.items()):\n",
    "                self.words_by_classes[cl] += val\n",
    "                self.vocab[tok] = 1\n",
    "        print(len(self.vocab))\n",
    "        self.vocab_len = len(self.vocab)\n",
    "\n",
    "        self.get_classes_probabilites_log(self.Y)\n",
    "        \n",
    "        self.denominators = dict()\n",
    "        print(\"Calculating classes denominators for probabilites:\")\n",
    "        for cl in tqdm(self.classes):\n",
    "            self.denominators[cl]  = self.words_by_classes[cl] + self.vocab_len\n",
    "        \n",
    "        self.Y_info = [(self.BoT[cl], self.classes_proba[cl], self.denominators[cl]) for cl in self.classes] \n",
    "        self.Y_info = np.array(self.Y_info) \n",
    "\n",
    "    def predict(self, text):\n",
    "        likelihood_prob = np.zeros(self.classes.shape[0])\n",
    "        if self.preProcess == True:\n",
    "            test_str = self.textPreProcessor.cleanText(text)\n",
    "        for cl_i, cl in enumerate(self.classes):                 \n",
    "            for tok in self.nGram.ngram(text):                        \n",
    "                tok_counts = self.Y_info[cl_i][0].get(tok, 0) + 1 # We add 1 due to the formula to not get 0 probabilities                        \n",
    "                tok_prob = tok_counts/float(self.Y_info[cl_i][2])                              \n",
    "                likelihood_prob[cl_i] += math.log(tok_prob)\n",
    "                                                \n",
    "        post_prob = np.empty(self.classes.shape[0])\n",
    "        for cl_i, cl in enumerate(self.classes):\n",
    "            post_prob[cl_i] = likelihood_prob[cl_i] + self.Y_info[cl_i][1]                              \n",
    "        \n",
    "        return post_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "textCleaner = TextPreProcessor( \n",
    "    lemmatization=False,\n",
    "    lower=True,\n",
    "    stopword = False, \n",
    "    stemmatization = True, \n",
    "    ponct = True, \n",
    "    emoji = True, \n",
    "    symbols = True, \n",
    "    numbers = True)\n",
    "nb = Nb_ngram(ngram=3, textPreProcessor=textCleaner, preProcess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    shuffle=True, \n",
    "    stratify=df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def oversampling(df,n):\n",
    "#     tokenizer = NGram(n);\n",
    "#     average_input_size = df[\"short_description\"].apply(lambda x: len(x.split(\" \"))).describe()[\"50%\"]\n",
    "#     max_size = df.category.value_counts()[0]\n",
    "\n",
    "#     for classe in np.unique(df.category):\n",
    "#         classe_vocab = []\n",
    "#         for sentences in df[df[\"category\"] == classe].short_description.values:\n",
    "#             classe_vocab += tokenizer.ngram(sentences)\n",
    "#         classe_vocab = np.unique(classe_vocab)\n",
    "#         gen_nb = max_size - df.category.value_counts()[classe]\n",
    "#         print(f\"Oversampling the classe {classe}:\")\n",
    "#         arr = []\n",
    "#         for i in tqdm(range(gen_nb)):\n",
    "#             random_tokens = np.random.choice(classe_vocab, size=int(average_input_size/3), replace=True)\n",
    "#             new_string = \"\"\n",
    "#             index = 0\n",
    "#             for i in random_tokens:\n",
    "#                 new_string += i \n",
    "#                 if index != max_size-1:\n",
    "#                     new_string += \" \"\n",
    "#                 index += 1\n",
    "#             arr = np.append(arr, {\"cat\": classe, \"text\": new_string})\n",
    "#         print(arr)\n",
    "#         df2 = pd.DataFrame.from_records([{\"category\": dict_value.get(\"cat\"), \"short_description\": dict_value.get(\"text\")} for dict_value in arr])\n",
    "#         df = pd.concat([df, df2])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = oversampling(train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167621/167621 [00:29<00:00, 5773.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe ARTS & CULTURE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25343/25343 [00:00<00:00, 34572.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe BUSINESS & FINANCES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22283/22283 [00:00<00:00, 39299.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe COMEDY:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24161/24161 [00:00<00:00, 39426.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe CRIME:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25631/25631 [00:00<00:00, 39431.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe DIVORCE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25740/25740 [00:00<00:00, 40427.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe EDUCATION:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26755/26755 [00:00<00:00, 39930.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe ENTERTAINMENT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14592/14592 [00:00<00:00, 39406.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe ENVIRONMENT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25228/25228 [00:00<00:00, 38769.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe FOOD & DRINK:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21732/21732 [00:00<00:00, 39301.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe GROUPS VOICES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18833/18833 [00:00<00:00, 38043.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe HOME & LIVING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25025/25025 [00:00<00:00, 40559.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe IMPACT:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25694/25694 [00:00<00:00, 38754.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe MEDIA:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26126/26126 [00:00<00:00, 39883.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe MISCELLANEOUS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26242/26242 [00:00<00:00, 39760.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe PARENTING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18284/18284 [00:00<00:00, 33664.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe POLITICS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe RELIGION:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26419/26419 [00:00<00:00, 39431.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe SCIENCE & TECH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25033/25033 [00:00<00:00, 40243.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe SPORTS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24419/24419 [00:00<00:00, 39259.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe STYLE & BEAUTY:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18827/18827 [00:00<00:00, 38266.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe TRAVEL:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20561/20561 [00:00<00:00, 39015.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe U.S. NEWS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27379/27379 [00:00<00:00, 38980.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WEDDINGS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25559/25559 [00:00<00:00, 39874.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WEIRD NEWS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26259/26259 [00:00<00:00, 40775.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WELLNESS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8770/8770 [00:00<00:00, 36084.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WOMEN:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25623/25623 [00:00<00:00, 39238.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling the classe WORLD NEWS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20848/20848 [00:00<00:00, 39407.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bags of words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768987it [00:09, 79969.29it/s]\n"
     ]
    }
   ],
   "source": [
    "nb.compile(train.short_description.values, train.category.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting vocab:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293860/293860 [00:00<00:00, 1708626.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331328/331328 [00:00<00:00, 1632151.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274428/274428 [00:00<00:00, 1614302.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273884/273884 [00:00<00:00, 2489867.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300001/300001 [00:00<00:00, 2438221.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290721/290721 [00:00<00:00, 2462020.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 273734/273734 [00:00<00:00, 1471609.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297476/297476 [00:00<00:00, 2110450.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298588/298588 [00:00<00:00, 1388778.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318305/318305 [00:00<00:00, 1041723.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288079/288079 [00:01<00:00, 285163.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 305474/305474 [00:00<00:00, 1432229.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275693/275693 [00:00<00:00, 1766456.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292816/292816 [00:00<00:00, 370835.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382843/382843 [00:00<00:00, 764798.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 325568/325568 [00:33<00:00, 9662.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325568\n"
     ]
    }
   ],
   "source": [
    "nb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X,Y):\n",
    "    true_possitive = 0\n",
    "    for lb, text in tqdm(zip(Y, X), total=len(Y)):\n",
    "        predict = nb.classes[nb.predict(text).argmax()]\n",
    "        if predict == lb:\n",
    "            true_possitive += 1\n",
    "\n",
    "    return float(true_possitive) / float(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve better accuracy when we do not remove the stop words. The accuracy is at its best when ngram=3.\n",
    "The accuracy is still pretty low due to an umbalanced dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41906/41906 [00:41<00:00, 1021.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is : 0.18049921252326637 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(test.short_description.values, test.category.values)\n",
    "print(f\"Model accuracy is : {accuracy} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f56dc8f305b8407d8b48cc33503d84dbeabbbd03fe8c646d69c42e169c35af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
